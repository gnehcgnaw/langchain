{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LangChain、GPT和Deep Lake处理代码库\n",
    "在本教程中，我们将使用LangChain+Deep Lake和GPT来分析LangChain自身的代码库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 数据准备：\n",
    "   1. 使用`langchain.document_loaders.TextLoader`上传所有Python项目文件。我们称这些文件为“文档”。\n",
    "   2. 使用`langchain.text_splitter.CharacterTextSplitter`将所有文档分成块。\n",
    "   3. 使用`langchain.embeddings.openai.OpenAIEmbeddings`和`langchain.vectorstores.DeepLake`将块嵌入并上传到DeepLake中。\n",
    "2. 问答：\n",
    "   1. 从`langchain.chat_models.ChatOpenAI`和`langchain.chains.ConversationalRetrievalChain`构建链。\n",
    "   2. 准备问题。\n",
    "   3. 运行链以获取答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Integration preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up keys for external services and install necessary python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T06:32:34.992175Z",
     "start_time": "2023-05-10T06:32:34.926067Z"
    }
   },
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade langchain deeplake openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up OpenAI embeddings, Deep Lake multi-modal vector store api and authenticate. \n",
    "\n",
    "For full documentation of Deep Lake please follow https://docs.activeloop.ai/ and API reference https://docs.deeplake.ai/en/latest/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate into Deep Lake if you want to create your own dataset and publish it. You can get an API key from the platform at [app.activeloop.ai](https://app.activeloop.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: hub://gnehcgnaw/langchain-code\n"
     ]
    },
    {
     "data": {
      "text/plain": "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='2022-12-01', openai_api_base=None, openai_api_type=None, embedding_ctx_length=8191, openai_api_key=None, openai_organization=None, allowed_special=set(), disallowed_special=set(), chunk_size=1000, max_retries=6)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "org = os.getenv('ACTIVELOOP_ORG')\n",
    "os.environ['ACTIVELOOP_TOKEN'] = os.getenv('ACTIVELOOP_TOKEN')\n",
    "os.environ['ACTIVELOOP_ORG'] = org\n",
    "\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())\n",
    "dataset_path = 'hub://' + org + '/langchain-code'\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-10T06:39:25.642842Z",
     "start_time": "2023-05-10T06:39:25.618734Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all repository files. Here we assume this notebook is downloaded as the part of the langchain fork and we work with the python files of the `langchain` repo.\n",
    "\n",
    "If you want to use files from different repo, change `root_dir` to the root dir of your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T06:32:40.557086Z",
     "start_time": "2023-05-10T06:32:34.996360Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m: \n\u001B[1;32m     10\u001B[0m     loader \u001B[38;5;241m=\u001B[39m TextLoader(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dirpath, file), encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m     docs\u001B[38;5;241m.\u001B[39mextend(\u001B[43mloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_and_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e: \n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/WorkSpaces/OpenSourceProject/langchain/langchain/document_loaders/base.py:35\u001B[0m, in \u001B[0;36mBaseLoader.load_and_split\u001B[0;34m(self, text_splitter)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m     _text_splitter \u001B[38;5;241m=\u001B[39m text_splitter\n\u001B[0;32m---> 35\u001B[0m docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _text_splitter\u001B[38;5;241m.\u001B[39msplit_documents(docs)\n",
      "File \u001B[0;32m~/WorkSpaces/OpenSourceProject/langchain/langchain/document_loaders/text.py:18\u001B[0m, in \u001B[0;36mTextLoader.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"Load from file path.\"\"\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_path, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 18\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m metadata \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_path}\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [Document(page_content\u001B[38;5;241m=\u001B[39mtext, metadata\u001B[38;5;241m=\u001B[39mmetadata)]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/codecs.py:319\u001B[0m, in \u001B[0;36mBufferedIncrementalDecoder.decode\u001B[0;34m(self, input, final)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_buffer_decode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, errors, final):\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001B[39;00m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;66;03m# and return an (output, length consumed) tuple\u001B[39;00m\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n\u001B[0;32m--> 319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;66;03m# decode input (taking the buffer into account)\u001B[39;00m\n\u001B[1;32m    321\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[1;32m    322\u001B[0m     (result, consumed) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer_decode(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors, final)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = '../../../..'\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith('.py') and '/.venv/' not in dirpath:\n",
    "            try: \n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e: \n",
    "                pass\n",
    "print(f'{len(docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, chunk the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then embed chunks and upload them to the DeepLake.\n",
    "\n",
    "This can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T06:59:12.882333Z",
     "start_time": "2023-05-10T06:39:34.826922Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/gnehcgnaw/langchain-code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://gnehcgnaw/langchain-code loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 61%|██████▏   | 49/80 [08:56<05:56Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists..\n",
      "Evaluating ingest: 100%|██████████| 80/80 [17:16<00:00\n",
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://gnehcgnaw/langchain-code', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype       shape       dtype  compression\n",
      "  -------   -------     -------     -------  ------- \n",
      " embedding  generic  (81406, 1536)  float32   None   \n",
      "    ids      text     (81406, 1)      str     None   \n",
      " metadata    json     (81406, 1)      str     None   \n",
      "   text      text     (81406, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": "<langchain.vectorstores.deeplake.DeepLake at 0x7feb53bdff10>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可能出现的问题：https://github.com/hwchase17/langchain/issues/923\n",
    "# 解决办法：https://github.com/shashnkvats/PdfPal/issues/1\n",
    "from langchain.vectorstores import DeepLake\n",
    "#  deeplake token 默认是1天过期，需要重新获取\n",
    "DeepLake.force_delete_by_path(dataset_path)\n",
    "db = DeepLake.from_documents(texts, embeddings, dataset_path=dataset_path)\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "首先加载数据集，构建检索器，然后构建会话链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T09:16:50.029987Z",
     "start_time": "2023-05-10T09:15:45.858635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path=dataset_path, read_only=True, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T09:17:44.566571Z",
     "start_time": "2023-05-10T09:17:44.560619Z"
    }
   },
   "outputs": [],
   "source": [
    "# as_retriever() 是一个检索器，可以用来检索，也可以用来过滤\n",
    "retriever = db.as_retriever()\n",
    "# distance_metric: 'cos' or 'euclidean' ,cos是余弦距离，euclidean是欧式距离\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "# fetch_k: 检索时，每个检索结果返回的最大数量\n",
    "retriever.search_kwargs['fetch_k'] = 20\n",
    "# maximal_marginal_relevance: 是否使用MMR算法，用于检索结果的排序 ,mmr是一种排序算法，用于对检索结果进行排序\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "# k是MMR算法中，每个检索结果返回的最大数量\n",
    "retriever.search_kwargs['k'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify user defined functions using [Deep Lake filters](https://docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T09:17:48.710379Z",
     "start_time": "2023-05-10T09:17:48.704920Z"
    }
   },
   "outputs": [],
   "source": [
    "# 以下代码的意思是：过滤掉包含'something'的文档，过滤掉路径中包含'only_this'或'also_that'的文档\n",
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if 'something' in x['text'].data()['value']:\n",
    "        return False\n",
    "    \n",
    "    # filter based on path e.g. extension\n",
    "    metadata =  x['metadata'].data()['value']\n",
    "    return 'only_this' in metadata['source'] or 'also_that' in metadata['source']\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T09:18:13.134350Z",
     "start_time": "2023-05-10T09:18:13.117998Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo') # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-10T09:31:26.025268Z",
     "start_time": "2023-05-10T09:28:17.850757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: AIMessagePromptTemplate 如何使用？ \n",
      "\n",
      "**Answer**: AIMessagePromptTemplate是一个用于生成机器人消息的模板类。您可以使用它来创建一个包含机器人消息的ChatPromptTemplate。这个模板类的使用方式类似于其他的PromptTemplate。您需要传入一个包含机器人消息的字符串模板，该模板中可以包含变量。在调用format方法时，您可以提供这些变量的值，以生成最终的机器人消息。 \n",
      "\n",
      "例如，假设您有一个包含机器人消息的ChatPromptTemplate，您可以使用AIMessagePromptTemplate来为这个模板生成机器人消息：\n",
      "\n",
      "```\n",
      "from langchain.prompts.chat import AIMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
      "\n",
      "prompt_template = ChatPromptTemplate(\n",
      "    [\n",
      "        HumanMessagePromptTemplate(\"What's your favorite color?\"),\n",
      "        AIMessagePromptTemplate(\"My favorite color is {color}.\")\n",
      "    ]\n",
      ")\n",
      "\n",
      "prompt = prompt_template.format(color=\"blue\")\n",
      "```\n",
      "\n",
      "在这个例子中，AIMessagePromptTemplate用于生成机器人消息，其中包含了变量{color}，在调用format方法时，您必须传入color参数的值，以便将变量替换为实际的值。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"AIMessagePromptTemplate 如何使用？\",\n",
    "    # \"What classes are derived from the Chain class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-> **Question**: What is the class hierarchy? \n",
    "\n",
    "**Answer**: There are several class hierarchies in the provided code, so I'll list a few:\n",
    "\n",
    "1. `BaseModel` -> `ConstitutionalPrinciple`: `ConstitutionalPrinciple` is a subclass of `BaseModel`.\n",
    "2. `BasePromptTemplate` -> `StringPromptTemplate`, `AIMessagePromptTemplate`, `BaseChatPromptTemplate`, `ChatMessagePromptTemplate`, `ChatPromptTemplate`, `HumanMessagePromptTemplate`, `MessagesPlaceholder`, `SystemMessagePromptTemplate`, `FewShotPromptTemplate`, `FewShotPromptWithTemplates`, `Prompt`, `PromptTemplate`: All of these classes are subclasses of `BasePromptTemplate`.\n",
    "3. `APIChain`, `Chain`, `MapReduceDocumentsChain`, `MapRerankDocumentsChain`, `RefineDocumentsChain`, `StuffDocumentsChain`, `HypotheticalDocumentEmbedder`, `LLMChain`, `LLMBashChain`, `LLMCheckerChain`, `LLMMathChain`, `LLMRequestsChain`, `PALChain`, `QAWithSourcesChain`, `VectorDBQAWithSourcesChain`, `VectorDBQA`, `SQLDatabaseChain`: All of these classes are subclasses of `Chain`.\n",
    "4. `BaseLoader`: `BaseLoader` is a subclass of `ABC`.\n",
    "5. `BaseTracer` -> `ChainRun`, `LLMRun`, `SharedTracer`, `ToolRun`, `Tracer`, `TracerException`, `TracerSession`: All of these classes are subclasses of `BaseTracer`.\n",
    "6. `OpenAIEmbeddings`, `HuggingFaceEmbeddings`, `CohereEmbeddings`, `JinaEmbeddings`, `LlamaCppEmbeddings`, `HuggingFaceHubEmbeddings`, `TensorflowHubEmbeddings`, `SagemakerEndpointEmbeddings`, `HuggingFaceInstructEmbeddings`, `SelfHostedEmbeddings`, `SelfHostedHuggingFaceEmbeddings`, `SelfHostedHuggingFaceInstructEmbeddings`, `FakeEmbeddings`, `AlephAlphaAsymmetricSemanticEmbedding`, `AlephAlphaSymmetricSemanticEmbedding`: All of these classes are subclasses of `BaseLLM`. \n",
    "\n",
    "\n",
    "-> **Question**: What classes are derived from the Chain class? \n",
    "\n",
    "**Answer**: There are multiple classes that are derived from the Chain class. Some of them are:\n",
    "- APIChain\n",
    "- AnalyzeDocumentChain\n",
    "- ChatVectorDBChain\n",
    "- CombineDocumentsChain\n",
    "- ConstitutionalChain\n",
    "- ConversationChain\n",
    "- GraphQAChain\n",
    "- HypotheticalDocumentEmbedder\n",
    "- LLMChain\n",
    "- LLMCheckerChain\n",
    "- LLMRequestsChain\n",
    "- LLMSummarizationCheckerChain\n",
    "- MapReduceChain\n",
    "- OpenAPIEndpointChain\n",
    "- PALChain\n",
    "- QAWithSourcesChain\n",
    "- RetrievalQA\n",
    "- RetrievalQAWithSourcesChain\n",
    "- SequentialChain\n",
    "- SQLDatabaseChain\n",
    "- TransformChain\n",
    "- VectorDBQA\n",
    "- VectorDBQAWithSourcesChain\n",
    "\n",
    "There might be more classes that are derived from the Chain class as it is possible to create custom classes that extend the Chain class.\n",
    "\n",
    "\n",
    "-> **Question**: What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests? \n",
    "\n",
    "**Answer**: All classes and functions in the `./langchain/utilities/` folder seem to have unit tests written for them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
